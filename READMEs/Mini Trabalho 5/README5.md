# üß† Mini Trabalho 5 ‚Äî Sele√ß√£o Inicial de Modelos de Machine Learning

## Equipe:

- Andre Ricardo Meyer de Melo  - 231011097
- Davi Rodrigues da Rocha - 211061618
- Luiz Felipe Bessa Santos - 231011687
- Tiago Antunes Balieiro - 231011838
- Wesley Pedrosa dos Santos - 180029240 

## üéØ Objetivo

Neste trabalho, avaliamos diferentes modelos de aprendizado de m√°quina com o intuito de prever a **aprova√ß√£o de proposi√ß√µes legislativas** com base em informa√ß√µes sobre o deputado, proposi√ß√£o, partido, orienta√ß√£o, tema, entre outros fatores.

Utilizamos o dataset consolidado previamente no Mini Trabalho 4 (`df_consolidado.csv`), que re√∫ne dados de votos individuais, proposi√ß√µes, orienta√ß√µes partid√°rias, autores e temas, todos referentes √† 56¬™ legislatura (2019‚Äì2022).

---

## ‚öôÔ∏è Modelos Avaliados

Foram testados cinco modelos de classifica√ß√£o, variando entre simples e mais sofisticados:

- **Decision Tree**
- **Random Forest**
- **K-Nearest Neighbors (KNN)**
- **Support Vector Machine (SVM)**
- **XGBoost (Gradient Boosting)**

Cada modelo foi treinado utilizando uma pipeline com pr√©-processamento adequado, incluindo:

- Codifica√ß√£o de vari√°veis categ√≥ricas (como `siglaUf` e `tema`)
- Tratamento de valores ausentes
- Balanceamento de classes com `class_weight`
- Normaliza√ß√£o (quando necess√°rio, como em KNN e SVM)

---

## üß™ M√©tricas de Avalia√ß√£o

Utilizamos as seguintes m√©tricas para compara√ß√£o:

- **Acur√°cia**
- **Precis√£o**
- **Recall**
- **F1-Score**
- (Opcional) **AUC-ROC** para XGBoost

Essas m√©tricas foram obtidas a partir de `classification_report` e `accuracy_score` da biblioteca `sklearn`.

---

## üìä Resultados Resumidos

| Modelo                 | Acur√°cia | F1-Score (Classe 1) | Observa√ß√µes                                    |
|------------------------|----------|----------------------|------------------------------------------------|
| Random Forest          | 0.93     | ~0.96                | Melhor desempenho geral                        |
| Decision Tree          | 0.91     | ~0.94                | Boa interpretabilidade, desempenho robusto     |
| K-Nearest Neighbors    | 0.89     | ~0.92                | Requer normaliza√ß√£o, desempenho aceit√°vel      |
| Support Vector Machine | 0.90     | ~0.93                | Performance competitiva, mais lento            |
| XGBoost                | 0.94     | ~0.96+               | Melhor AUC-ROC, ideal para melhorias futuras   |

> **Nota**: Os valores s√£o aproximados e podem variar com novos ajustes de hiperpar√¢metros.

---

## üîç An√°lise Cr√≠tica

- Os **modelos de √°rvore** se destacaram tanto em desempenho quanto em interpretabilidade.
- O **XGBoost** mostrou o melhor desempenho, especialmente em datasets desbalanceados, e deve ser aprofundado futuramente.
- **KNN e SVM** requerem mais tempo de treino e cuidado com escalonamento, por√©m mostraram bom desempenho.
- O balanceamento de classes e codifica√ß√£o correta das vari√°veis categ√≥ricas foram essenciais para bons resultados.

---

## ‚úÖ Justificativa para os Modelos Selecionados

O modelo **Random Forest** foi o mais equilibrado em performance e tempo de treino. O **XGBoost**, apesar de mais complexo, mostrou maior potencial e ser√° foco para ajustes finos nos pr√≥ximos passos. Ambos foram escolhidos para continuidade do projeto.

---

## üìÅ Organiza√ß√£o do Notebook

O notebook `comparacao_modelos.ipynb` est√° organizado nas seguintes se√ß√µes:

1. **Introdu√ß√£o e M√©tricas**
2. **Tabela Comparativa de Resultados**
3. **Gr√°ficos de Acur√°cia e F1-Score**
4. **C√≥digo e Avalia√ß√£o por Modelo**
5. **Conclus√£o e Pr√≥ximos Passos**

---

## üìå Pr√≥ximos Passos

- Otimiza√ß√£o dos hiperpar√¢metros (ex: GridSearchCV)
- An√°lise mais detalhada dos erros de classifica√ß√£o
- Implementa√ß√£o de t√©cnicas de oversampling (ex: SMOTE)
- Deploy ou salvamento dos modelos para uso futuro (`joblib`)

---

